{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full experiments on Amazon tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from models.hdisc_msda import Disc_MSDANet, weighted_mse\n",
    "from models.MDAN import MDANet_general\n",
    "from utils.load_amazon import load_amazon\n",
    "from utils.utils_hdisc import batch_loader, split_source_target, val_split, save_result\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbdd4116690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading\n",
    "X_amazon, y_amazon, domain_list = load_amazon(domains=None)\n",
    "#Standardize labels\n",
    "mu_y = np.mean(np.concatenate(y_amazon))\n",
    "std_y = np.std(np.concatenate(y_amazon))\n",
    "y_amazon = [(y-mu_y)/std_y for y in y_amazon]\n",
    "#y_amazon = [y-3 for y in y_amazon]\n",
    "#Number of domains\n",
    "n_domains = len(X_amazon)\n",
    "print(n_domains)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor():\n",
    "    return nn.ModuleList([\n",
    "            nn.Linear(X_amazon[0].shape[1], 500, bias=False), nn.LeakyReLU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(500, 20, bias=False), nn.LeakyReLU(), nn.Dropout(p=0.1)])\n",
    "\n",
    "def get_predictor(output_dim=1):\n",
    "    return  nn.ModuleList([\n",
    "            #nn.Linear(500,100, bias=False), nn.ELU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(20, output_dim, bias=False)])\n",
    "\n",
    "def get_discriminator(output_dim=1):\n",
    "    return nn.ModuleList([\n",
    "            #nn.Linear(500, 100, bias=False), nn.ELU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(20, output_dim, bias=False)])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Merge Sources $\\rightarrow$ Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models.hdisc_msda \n",
    "importlib.reload(models.hdisc_msda)\n",
    "from models.hdisc_msda import Disc_MSDANet\n",
    "\n",
    "#Number of experiments to launch\n",
    "nb_experiments = 5\n",
    "results_mse, results_mae = [], []\n",
    "\n",
    "params= {'input_dim': X_amazon[0].shape[1], 'output_dim': 1, 'n_sources': n_domains-1, 'loss': torch.nn.MSELoss(),\n",
    "         'weighted_loss': weighted_mse, 'min_pred': -np.inf, 'max_pred': np.inf}\n",
    "#Number of epochs\n",
    "epochs = 100\n",
    "device = torch.device('cuda:0')\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "keep_best = True\n",
    "current_loss = 10e9\n",
    "for exp in range(nb_experiments):\n",
    "    print('\\n ----------------------------- %i / %i -----------------------------'%(exp+1, nb_experiments))\n",
    "    mse_list, mae_list =  {}, {}\n",
    "    for i in range(len(domain_list)):\n",
    "        domain = domain_list[i]\n",
    "        #Split source and target\n",
    "        torch.cuda.empty_cache() \n",
    "        X_s, X_t, y_s, y_t = split_source_target(X_amazon, y_amazon, i, device, merge=False)\n",
    "        #Validation split\n",
    "        X_train, X_val, y_train, y_val = val_split(X_s, y_s)\n",
    "        #Initialize model\n",
    "        params['feature_extractor'] = get_feature_extractor()\n",
    "        params['h_pred'] = get_predictor(output_dim=1)\n",
    "        params['h_disc'] = get_discriminator(output_dim=1)\n",
    "        model = Disc_MSDANet(params).to(device)\n",
    "        opt_feat = torch.optim.Adam([{'params': model.feature_extractor.parameters()}],lr=lr)\n",
    "        opt_pred = torch.optim.Adam([{'params': model.h_pred.parameters()}],lr=lr)\n",
    "        opt_disc =torch.optim.Adam([{'params': model.h_disc.parameters()}],lr=lr)\n",
    "        opt_alpha =torch.optim.Adam([{'params': model.alpha}],lr=lr)\n",
    "        model.optimizers(opt_feat, opt_pred, opt_disc, opt_alpha)\n",
    "        print('----', domain, '----')\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loader = batch_loader(X_train, y_train ,batch_size = batch_size)\n",
    "            for x_bs, y_bs in loader:\n",
    "                loss_pred = model.train_prediction(x_bs, X_t, y_bs, clip=1, pred_only=False)\n",
    "            #Validation\n",
    "            model.eval()\n",
    "            val_loss, _ = model.compute_loss(X_val, X_t, y_val)\n",
    "            if (val_loss.item()<current_loss)*(keep_best):\n",
    "                current_loss = val_loss.item()\n",
    "            if (epoch+1)%100==0:\n",
    "                model.eval()\n",
    "                source_loss, disc = model.compute_loss(X_s, X_t, y_s)\n",
    "                reg_loss = model.loss(y_t, model.predict(X_t))\n",
    "                print('Epoch: %i/%i ; Train loss: %.3f ; Validation loss: %.3f Test loss: %.3f'%(epoch+1, epochs, source_loss.item(), val_loss.item(), reg_loss.item()))\n",
    "        mse_list[domain] = model.loss(y_t, model.predict(X_t)).item()\n",
    "        mae_list[domain] = torch.sum(torch.abs(y_t.squeeze_()- model.predict(X_t).squeeze_()))/y_t.shape[0]\n",
    "    results_mse.append(mse_list)\n",
    "    results_mae.append(mae_list)\n",
    "save_result(results_mse, './results/1SRC_noadapt_mse.csv', domain_list)\n",
    "save_result(results_mae, './results/1SRC_noadapt_mae.csv', domain_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DANN - Merge Sources $\\rightarrow$ Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Number of experiments to launch\n",
    "nb_experiments = 5\n",
    "results_mse, results_mae = [], []\n",
    "\n",
    "configs = {\"num_epochs\": 100, \"num_domains\": n_domains-1, \n",
    "           \"mode\": 'DANN', \"verbose\": 2}\n",
    "\n",
    "#Number of epochs\n",
    "epochs = 100\n",
    "device = torch.device('cuda:0')\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "#Hyperparameters\n",
    "mu = 0.01\n",
    "gamma = 10\n",
    "mode = configs['mode']\n",
    "\n",
    "for exp in range(nb_experiments):\n",
    "    print('\\n ----------------------------- %i / %i -----------------------------'%(exp+1, nb_experiments))\n",
    "    mse_list, mae_list =  {}, {}\n",
    "    for i in range(len(domain_list)):\n",
    "        domain = domain_list[i]\n",
    "        #Split source and target\n",
    "        torch.cuda.empty_cache() \n",
    "        X_s, X_t, y_s, y_t = split_source_target(X_amazon, y_amazon, i, device, merge=False)\n",
    "        #Initialize model\n",
    "        configs['hiddens'] = get_feature_extractor()\n",
    "        configs['predictor'] = get_predictor(output_dim=1)\n",
    "        configs['discriminator'] = [get_discriminator(output_dim=2) for _ in range(configs['num_domains'])]\n",
    "        mdan = MDANet_general(configs).to(device)\n",
    "        for i in range(configs['num_domains']):\n",
    "            mdan.discriminator[i].to(device)\n",
    "        optimizer = torch.optim.Adam(mdan.parameters(), lr=lr)\n",
    "        print('----', domain, '----')\n",
    "        for epoch in range(epochs):\n",
    "            mdan.train()\n",
    "            loader = batch_loader(X_s, y_s ,batch_size = batch_size)\n",
    "            for x_bs, y_bs in loader:\n",
    "                slabels = torch.ones(batch_size, requires_grad=False).type(torch.LongTensor).to(device)\n",
    "                tlabels = torch.zeros(batch_size, requires_grad=False).type(torch.LongTensor).to(device)\n",
    "                ridx = np.random.choice(X_t.shape[0], batch_size)\n",
    "                #Batch selection of X_t\n",
    "                tinputs = X_t[ridx, :]\n",
    "                optimizer.zero_grad()\n",
    "                vals, sdomains, tdomains = mdan(x_bs, tinputs)\n",
    "                # Compute prediction accuracy on multiple training sources.\n",
    "                losses = torch.stack([F.mse_loss(vals[j], y_bs[j]) for j in range(configs['num_domains'])])\n",
    "                domain_losses = torch.stack([F.nll_loss(sdomains[j], slabels) +\n",
    "                                           F.nll_loss(tdomains[j], tlabels) for j in range(configs['num_domains'])])\n",
    "                # Different final loss function depending on different training modes.\n",
    "                if mode == \"maxmin\":\n",
    "                    loss = torch.max(losses) + mu * torch.min(domain_losses)\n",
    "                elif mode == \"dynamic\":\n",
    "                    loss = torch.log(torch.sum(torch.exp(gamma * (losses + mu * domain_losses)))) / gamma\n",
    "                elif mode=='no-weight':\n",
    "                    loss = torch.mean(losses)\n",
    "                elif mode=='DANN':\n",
    "                    loss = torch.mean(losses) + mu * torch.mean(domain_losses)\n",
    "                loss.backward(retain_graph=True )\n",
    "                optimizer.step()\n",
    "            if (epoch+1)%100==0:\n",
    "                mdan.eval()\n",
    "                preds_labels = mdan.inference(X_t)\n",
    "                source_loss = torch.mean(torch.stack([torch.nn.MSELoss()(y_s[i], mdan.inference(X_s[i])) for i in range(len(y_s))]))\n",
    "                print('Epoch: %i ; Train loss: %.3f ;  Loss: %.3f'%(epoch+1, source_loss, torch.nn.MSELoss()(y_t, preds_labels)))\n",
    "        mse_list[domain] = torch.nn.MSELoss()(y_t, mdan.inference(X_t)).item()\n",
    "        mae_list[domain] = torch.sum(torch.abs(y_t.squeeze_()- mdan.inference(X_t).squeeze_()))/y_t.shape[0]\n",
    "    results_mse.append(mse_list)\n",
    "    results_mae.append(mae_list)\n",
    "    \n",
    "save_result(results_mse, './results/1SRC_DANN_mse.csv', domain_list)\n",
    "save_result(results_mae, './results/1SRC_DANN_mae.csv', domain_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ADisc-DA Merge Sources $\\rightarrow$ Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Number of experiments to launch\n",
    "nb_experiments = 5\n",
    "results_mse, results_mae = [], []\n",
    "\n",
    "params= {'input_dim': X_amazon[0].shape[1], 'output_dim': 1, 'n_sources': n_domains-1, 'loss': torch.nn.MSELoss(),\n",
    "         'weighted_loss': weighted_mse, 'min_pred': -np.inf, 'max_pred': np.inf}\n",
    "#Number of epochs\n",
    "epochs_pretrain = 0\n",
    "epochs_adapt = 100\n",
    "epochs_h_disc, epochs_feat, epochs_alpha, epochs_pred = 1, 1, 1, 1\n",
    "device = torch.device('cuda:1')\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "for exp in range(nb_experiments):\n",
    "    print('\\n ----------------------------- %i / %i -----------------------------'%(exp+1, nb_experiments))\n",
    "    mse_list, mae_list =  {}, {}\n",
    "    for i in range(len(domain_list)):\n",
    "        domain = domain_list[i]\n",
    "        #Split source and target\n",
    "        torch.cuda.empty_cache() \n",
    "        X_s, X_t, y_s, y_t = split_source_target(X_amazon, y_amazon, i, device, merge=False)\n",
    "        #Merge all sources\n",
    "        #Initialize model\n",
    "        params['feature_extractor'] = get_feature_extractor()\n",
    "        params['h_pred'] = get_predictor(output_dim=1)\n",
    "        params['h_disc'] = get_discriminator(output_dim=1)\n",
    "        model = Disc_MSDANet(params).to(device)\n",
    "        opt_feat = torch.optim.Adam([{'params': model.feature_extractor.parameters()}],lr=lr)\n",
    "        opt_pred = torch.optim.Adam([{'params': model.h_pred.parameters()}],lr=lr)\n",
    "        opt_disc =torch.optim.Adam([{'params': model.h_disc.parameters()}],lr=lr)\n",
    "        opt_alpha =torch.optim.Adam([{'params': model.alpha}],lr=lr)\n",
    "        model.optimizers(opt_feat, opt_pred, opt_disc, opt_alpha)\n",
    "        print('----', domain, '----')\n",
    "        #Pre-training\n",
    "        print('------------Pre-training------------')\n",
    "        for epoch in range(epochs_pretrain):\n",
    "            loader = batch_loader(X_s, y_s ,batch_size = batch_size)\n",
    "            for x_bs, y_bs in loader:\n",
    "                loss_pred = model.train_prediction(x_bs, X_t, y_bs, clip=1, pred_only=False)\n",
    "            if (epoch+1)%10==0:\n",
    "                source_loss, disc = model.compute_loss(X_s, X_t, y_s)\n",
    "                reg_loss = model.loss(y_t, model.predict(X_t))\n",
    "                print('Epoch: %i/%i ; Train loss: %.3f ; Disc: %.3f ; Test loss: %.3f'%(epoch+1, epochs_pretrain, source_loss.item(), disc.item(), reg_loss.item()))\n",
    "\n",
    "        #Alternated training\n",
    "        print('------------Alternated training------------')\n",
    "        for epoch in range(epochs_adapt):\n",
    "            loader = batch_loader(X_s, y_s ,batch_size = batch_size)\n",
    "            for x_bs, y_bs in loader:\n",
    "                ridx = np.random.choice(X_t.shape[0], batch_size)\n",
    "                x_bt = X_t[ridx,:]\n",
    "                #Train h to minimize source loss\n",
    "                for e in range(epochs_pred):\n",
    "                    model.train_prediction(x_bs, x_bt, y_bs, pred_only=False)\n",
    "                \n",
    "                #Train h' to maximize discrepancy\n",
    "                for e in range(epochs_h_disc):\n",
    "                    model.train_h_discrepancy(x_bs, x_bt, y_bs)\n",
    "\n",
    "                #Train phi to minimize discrepancy\n",
    "                for e in range(epochs_feat):\n",
    "                    model.train_feat_discrepancy(x_bs, x_bt, y_bs, mu=0.001)\n",
    "                \n",
    "            #Logs\n",
    "            if (epoch+1)%100==0:\n",
    "                source_loss, disc = model.compute_loss(X_s, X_t, y_s)\n",
    "                reg_loss = model.loss(y_t, model.predict(X_t))\n",
    "                print('Epoch: %i/%i (h_pred); Train loss: %.3f ; Disc: %.3f ; Test loss: %.3f'%(epoch+1, epochs_adapt, source_loss.item(), disc.item(), reg_loss.item()))\n",
    "        mse_list[domain] = model.loss(y_t, model.predict(X_t)).item()\n",
    "        mae_list[domain] = torch.sum(torch.abs(y_t.squeeze_()- model.predict(X_t).squeeze_()))/y_t.shape[0]\n",
    "    results_mse.append(mse_list)\n",
    "    results_mae.append(mae_list)\n",
    "    \n",
    "save_result(results_mse, './results/1SRC_Adisc_mse.csv', domain_list)\n",
    "save_result(results_mae, './results/1SRC_Adisc_mae.csv', domain_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MDAN - All Sources $\\rightarrow$ Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Number of experiments to launch\n",
    "nb_experiments = 5\n",
    "results_mse, results_mae = [], []\n",
    "\n",
    "configs = {\"num_domains\": len(domain_list)-1, \n",
    "           \"mode\": 'dynamic', \"verbose\": 2}\n",
    "\n",
    "#Number of epochs\n",
    "epochs = 100\n",
    "device = torch.device('cuda:1')\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "#Hyperparameters\n",
    "mu = 0.1\n",
    "gamma = 10\n",
    "mode = 'dynamic'\n",
    "\n",
    "for exp in range(nb_experiments):\n",
    "    print('\\n ----------------------------- %i / %i -----------------------------'%(exp+1, nb_experiments))\n",
    "    mse_list, mae_list =  {}, {}\n",
    "    for i in range(len(domain_list)):\n",
    "        domain = domain_list[i]\n",
    "        #Split source and target\n",
    "        torch.cuda.empty_cache() \n",
    "        X_s, X_t, y_s, y_t = split_source_target(X_amazon, y_amazon, i, device, merge=False)\n",
    "        #Initialize model\n",
    "        configs['hiddens'] = get_feature_extractor()\n",
    "        configs['predictor'] = get_predictor(output_dim=1)\n",
    "        configs['discriminator'] = [get_discriminator(output_dim=2) for _ in range(len(domain_list)-1)]\n",
    "        mdan = MDANet_general(configs).to(device)\n",
    "        for i in range(configs['num_domains']):\n",
    "            mdan.discriminator[i].to(device)\n",
    "        optimizer = torch.optim.Adam(mdan.parameters(), lr=lr)\n",
    "        print('----', domain, '----')\n",
    "        for epoch in range(epochs):\n",
    "            mdan.train()\n",
    "            loader = batch_loader(X_s, y_s ,batch_size = batch_size)\n",
    "            for x_bs, y_bs in loader:\n",
    "                slabels = torch.ones(batch_size, requires_grad=False).type(torch.LongTensor).to(device)\n",
    "                tlabels = torch.zeros(batch_size, requires_grad=False).type(torch.LongTensor).to(device)\n",
    "                ridx = np.random.choice(X_t.shape[0], batch_size)\n",
    "                #Batch selection of X_t\n",
    "                tinputs = X_t[ridx, :]\n",
    "                optimizer.zero_grad()\n",
    "                vals, sdomains, tdomains = mdan(x_bs, tinputs)\n",
    "                # Compute prediction accuracy on multiple training sources.\n",
    "                losses = torch.stack([F.mse_loss(vals[j], y_bs[j]) for j in range(configs['num_domains'])])\n",
    "                domain_losses = torch.stack([F.nll_loss(sdomains[j], slabels) +\n",
    "                                           F.nll_loss(tdomains[j], tlabels) for j in range(configs['num_domains'])])\n",
    "                # Different final loss function depending on different training modes.\n",
    "                if mode == \"maxmin\":\n",
    "                    loss = torch.max(losses) + mu * torch.min(domain_losses)\n",
    "                elif mode == \"dynamic\":\n",
    "                    loss = torch.log(torch.sum(torch.exp(gamma * (losses + mu * domain_losses)))) / gamma\n",
    "                elif mode=='no-weight':\n",
    "                    loss = torch.mean(losses)\n",
    "                elif mode=='DANN':\n",
    "                    loss = torch.mean(losses) + mu * torch.mean(domain_losses)\n",
    "                loss.backward(retain_graph=True )\n",
    "                optimizer.step()\n",
    "            if (epoch+1)%100==0:\n",
    "                mdan.eval()\n",
    "                preds_labels = mdan.inference(X_t)\n",
    "                source_loss = torch.mean(torch.stack([torch.nn.MSELoss()(y_s[i], mdan.inference(X_s[i])) for i in range(len(y_s))]))\n",
    "                print('Epoch: %i ; Train loss: %.3f ;  Loss: %.3f'%(epoch+1, source_loss, torch.nn.MSELoss()(y_t, preds_labels)))\n",
    "        mse_list[domain] = torch.nn.MSELoss()(y_t, mdan.inference(X_t)).item()\n",
    "        mae_list[domain] = torch.sum(torch.abs(y_t.squeeze_()- mdan.inference(X_t).squeeze_()))/y_t.shape[0]\n",
    "    results_mse.append(mse_list)\n",
    "    results_mae.append(mae_list)\n",
    "    \n",
    "save_result(results_mse, './results/MDAN_mse.csv', domain_list)\n",
    "save_result(results_mae, './results/MDAN_mae.csv', domain_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_alpha = np.zeros((len(domain_list), len(domain_list)))\n",
    "                   \n",
    "for i in range(len(domain_list)):\n",
    "    z_alpha[i,:i] = alphas[domain_list[i]][:i]\n",
    "    z_alpha[i,i+1:] = alphas[domain_list[i]][i:]\n",
    "\n",
    "plt.figure(figsize=(24,24))\n",
    "plt.imshow(1-z_alpha, cmap='gray')\n",
    "plt.xticks(np.arange(len(domain_list)), [d[:4] for d in domain_list], fontsize=14)\n",
    "plt.yticks(np.arange(len(domain_list)), domain_list, fontsize=14)\n",
    "plt.savefig('./plots/alpha_msda_amazon.jpg', transparent=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dds]",
   "language": "python",
   "name": "conda-env-dds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
